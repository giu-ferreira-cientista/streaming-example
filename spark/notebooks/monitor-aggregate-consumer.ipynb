{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('csv-changes-event-consumer')\n",
    "         # Add kafka package\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stream dataframe setting kafka server, topic and offset option\n",
    "df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") # kafka server ip address inspect - something like 172.23.0.5\n",
    "  .option(\"subscribe\", \"patient-data\") # topic\n",
    "  .option(\"startingOffsets\", \"earliest\") # start from beginning \n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Convert binary to string key and value\n",
    "df1 = (df\n",
    "    .withColumn(\"key\", df[\"key\"].cast(StringType()))\n",
    "    .withColumn(\"value\", df[\"value\"].cast(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print to jupyter output in two fases - memory and display\n",
    "query = df1 \\\n",
    "    .withWatermark(\"timestamp\", \"3 minutes\") \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"csv_query\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Getting offsets from KafkaV2[Subscribe[patient-data]]',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------+---------+------+--------------------+-------------+\n",
      "| key|               value|       topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+------------+---------+------+--------------------+-------------+\n",
      "|null|                  {}|patient-data|        0|     0|2022-12-09 12:05:...|            0|\n",
      "|   3|{\"id\":3,\"nome\":\"a...|patient-data|        0|     1|2022-12-09 12:05:...|            0|\n",
      "|   3|{\"id\":3,\"nome\":\"a...|patient-data|        0|     2|2022-12-09 12:05:...|            0|\n",
      "|   1|{\"id\":1,\"nome\":\"j...|patient-data|        0|     3|2022-12-09 12:05:...|            0|\n",
      "|   3|{\"id\":3,\"nome\":\"a...|patient-data|        0|     4|2022-12-09 12:05:...|            0|\n",
      "|   2|{\"id\":2,\"nome\":\"m...|patient-data|        0|     5|2022-12-09 12:05:...|            0|\n",
      "|   2|{\"id\":2,\"nome\":\"m...|patient-data|        0|     6|2022-12-09 12:05:...|            0|\n",
      "|   1|{\"id\":1,\"nome\":\"j...|patient-data|        0|     7|2022-12-09 12:05:...|            0|\n",
      "|   2|{\"id\":2,\"nome\":\"m...|patient-data|        0|     8|2022-12-09 12:05:...|            0|\n",
      "|   1|{\"id\":1,\"nome\":\"j...|patient-data|        0|     9|2022-12-09 12:05:...|            0|\n",
      "|null|                  {}|patient-data|        0|    10|2022-12-09 12:05:...|            0|\n",
      "|null|                  {}|patient-data|        0|    11|2022-12-09 12:12:...|            0|\n",
      "|   3|{\"id\":3,\"nome\":\"a...|patient-data|        0|    12|2022-12-09 12:12:...|            0|\n",
      "|   1|{\"id\":1,\"nome\":\"j...|patient-data|        0|    13|2022-12-09 12:12:...|            0|\n",
      "|   2|{\"id\":2,\"nome\":\"m...|patient-data|        0|    14|2022-12-09 12:12:...|            0|\n",
      "|   2|{\"id\":2,\"nome\":\"m...|patient-data|        0|    15|2022-12-09 12:12:...|            0|\n",
      "|   2|{\"id\":2,\"nome\":\"m...|patient-data|        0|    16|2022-12-09 12:12:...|            0|\n",
      "|   2|{\"id\":2,\"nome\":\"m...|patient-data|        0|    17|2022-12-09 12:12:...|            0|\n",
      "|   3|{\"id\":3,\"nome\":\"a...|patient-data|        0|    18|2022-12-09 12:12:...|            0|\n",
      "|   2|{\"id\":2,\"nome\":\"m...|patient-data|        0|    19|2022-12-09 12:12:...|            0|\n",
      "+----+--------------------+------------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a579930eb018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT * FROM csv_query'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m199\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# display the output\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    display(spark.sql('SELECT * FROM csv_query').show())\n",
    "    time.sleep(199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d005553bf1fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# alternative - print to console\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"console\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# alternative - print to console\n",
    "df1.writeStream \\\n",
    "   .format(\"console\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .start() \\\n",
    "   .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, LongType, IntegerType, DoubleType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Event data schema\n",
    "schema_csv = StructType(\n",
    "    [StructField(\"$schema\",StringType(),True),\n",
    "     StructField(\"id\", IntegerType(),True),\n",
    "     StructField(\"nome\", StringType(),True),\n",
    "     StructField(\"idade\", IntegerType(),True),\n",
    "     StructField(\"sexo\", IntegerType(),True),\n",
    "     StructField(\"peso\", DoubleType(),True),\n",
    "     StructField(\"bpm\", DoubleType(),True),\n",
    "     StructField(\"pressao\", DoubleType(),True),\n",
    "     StructField(\"respiracao\", DoubleType(),True),\n",
    "     StructField(\"temperatura\", DoubleType(),True),\n",
    "     StructField(\"glicemia\", DoubleType(),True),\n",
    "     StructField(\"saturacao_oxigenio\", DoubleType(),True),\n",
    "     StructField(\"estado_atividade\", IntegerType(),True),\n",
    "     StructField(\"dia_de_semana\", IntegerType(),True),\n",
    "     StructField(\"periodo_do_dia\", IntegerType(), True),\n",
    "     StructField(\"semana_do_mes\", IntegerType(),True),\n",
    "     StructField(\"estacao_do_ano\", IntegerType(),True),\n",
    "     StructField(\"passos\", IntegerType(),True),\n",
    "     StructField(\"calorias\", DoubleType(),True),\n",
    "     StructField(\"distancia\", DoubleType(),True),\n",
    "     StructField(\"tempo\", DoubleType(),True),\n",
    "     StructField(\"total_sleep_last_24\", DoubleType(),True),\n",
    "     StructField(\"deep_sleep_last_24\", DoubleType(),True),\n",
    "     StructField(\"light_sleep_last_24\", DoubleType(),True),\n",
    "     StructField(\"awake_last_24\", DoubleType(),True),\n",
    "     StructField(\"timestampstr\", TimestampType(),True)\n",
    "    ])\n",
    "\n",
    "# Create dataframe setting schema for event data\n",
    "df_csv = (df1\n",
    "           # Sets schema for event data\n",
    "           .withColumn(\"value\", from_json(\"value\", schema_csv))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f123f88c550>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print to console\n",
    "df_csv.writeStream \\\n",
    "   .format(\"console\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .start() \\\n",
    "   #.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_unixtime, to_date, to_timestamp, unix_timestamp\n",
    "\n",
    "# Transform into tabular \n",
    "# Convert unix timestamp to timestamp\n",
    "# Create partition column (change_timestamp_date)\n",
    "df_csv_formatted = (df_csv.select(\n",
    "    col(\"key\").alias(\"event_key\")\n",
    "    ,col(\"topic\").alias(\"event_topic\")\n",
    "    ,col(\"timestamp\").alias(\"event_timestamp\") \n",
    "    ,col(\"value.$schema\").alias(\"schema\")\n",
    "    ,col(\"value.id\").alias(\"id\")    \n",
    "    ,col(\"value.nome\").alias(\"nome\")    \n",
    "    ,col(\"value.idade\").alias(\"idade\")    \n",
    "    ,col(\"value.sexo\").alias(\"sexo\")    \n",
    "    ,col(\"value.bpm\").alias(\"bpm\")        \n",
    "    ,col(\"value.pressao\").alias(\"pressao\")        \n",
    "    ,col(\"value.respiracao\").alias(\"respiracao\")        \n",
    "    ,col(\"value.temperatura\").alias(\"temperatura\")        \n",
    "    ,col(\"value.glicemia\").alias(\"glicemia\")        \n",
    "    ,col(\"value.saturacao_oxigenio\").alias(\"saturacao_oxigenio\")        \n",
    "    ,col(\"value.estado_atividade\").alias(\"estado_atividade\")        \n",
    "    ,col(\"value.dia_de_semana\").alias(\"dia_de_semana\")        \n",
    "    ,col(\"value.periodo_do_dia\").alias(\"periodo_do_dia\")        \n",
    "    ,col(\"value.semana_do_mes\").alias(\"semana_do_mes\")        \n",
    "    ,col(\"value.estacao_do_ano\").alias(\"estacao_do_ano\")        \n",
    "    ,col(\"value.passos\").alias(\"passos\")        \n",
    "    ,col(\"value.calorias\").alias(\"calorias\")        \n",
    "    ,col(\"value.distancia\").alias(\"distancia\")        \n",
    "    ,col(\"value.total_sleep_last_24\").alias(\"total_sleep_last_24\")        \n",
    "    ,col(\"value.deep_sleep_last_24\").alias(\"deep_sleep_last_24\")            \n",
    "    ,col(\"value.light_sleep_last_24\").alias(\"light_sleep_last_24\")        \n",
    "    ,col(\"value.awake_last_24\").alias(\"awake_last_24\")    \n",
    "    ,to_timestamp(unix_timestamp(col(\"value.timestampstr\"))).alias(\"change_timestamp\")\n",
    "    ,to_date(col(\"value.timestampstr\")).alias(\"change_timestamp_date\")\n",
    "    ,col(\"value.timestampstr\").alias(\"change_timestamp_str\")         \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f12c4f75820>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print to console\n",
    "df_csv_formatted.writeStream \\\n",
    "   .format(\"console\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .start() \\\n",
    "   #.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start query stream over stream dataframe\n",
    "raw_path = \"/home/jovyan/work/data-lake/csv-changes\"\n",
    "checkpoint_path = \"/home/jovyan/work/data-lake/csv-changes-checkpoint\"\n",
    "\n",
    "queryStream =(\n",
    "    df_csv_formatted \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .queryName(\"csv_changes_ingestion-9\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .option(\"path\", raw_path) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .partitionBy(\"change_timestamp_date\", \"nome\") \\\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet files as stream to output the number of rows\n",
    "df_csv_changes = (\n",
    "    spark \\\n",
    "    .readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .schema(df_csv_formatted.schema) \\\n",
    "    .load(raw_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to memory to count rows\n",
    "queryStreamMem = (df_csv_changes\n",
    " .writeStream\n",
    " .format(\"memory\")\n",
    " .queryName(\"csv_changes_count3\")\n",
    " .outputMode(\"update\")\n",
    " .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:32\n",
      "stream process interrupted\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Count rows every 5 seconds while stream is active\n",
    "try:\n",
    "    i=1\n",
    "    # While stream is active, print count\n",
    "    while len(spark.streams.active) > 0:\n",
    "        \n",
    "        # Clear output\n",
    "        clear_output(wait=True)\n",
    "        print(\"Run:{}\".format(i))\n",
    "        \n",
    "        lst_queries = []\n",
    "        for s in spark.streams.active:\n",
    "            lst_queries.append(s.name)\n",
    "\n",
    "        # Verify if wiki_changes_count query is active before count\n",
    "        if \"csv_changes_count3\" in lst_queries:\n",
    "            # Count number of events\n",
    "            # spark.sql(\"select count(1) as qty from csv_changes_count\").show()\n",
    "            sql = spark.sql(\"select id, nome, count(*) as registros, avg(bpm) as avg_bpm, avg(pressao) as avg_pressao, avg(temperatura) as avg_temperatura, avg(respiracao) as avg_respiracao, count(passos) as count_passos, max(bpm) as max_bpm, max(temperatura) as max_temperatura, max(respiracao) as max_repiracao, min(bpm) as min_bpm, min(pressao) as min_pressao from csv_changes_count3 where id is not null group by id, nome order by nome\")\n",
    "            sql.show()\n",
    "            # TODO: insert into kafka aggregated topic \n",
    "        else:\n",
    "            print(\"'csv_changes_count' query not found.\")\n",
    "\n",
    "        sleep(5)\n",
    "        i=i+1\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    # Stop Query Stream\n",
    "    queryStreamMem.stop()\n",
    "    \n",
    "    print(\"stream process interrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:e1add6e2-d4a3-43a6-9e7f-7651c3a0820c | NAME:None\n",
      "ID:78a3af7e-05c2-4157-af46-fbe9a2cbd767 | NAME:None\n",
      "ID:65a39a51-4951-4774-a488-a9fa244ddbdf | NAME:None\n",
      "ID:8a920c92-4d7e-4db1-8997-3ad7f58b5a86 | NAME:csv_changes_ingestion-9\n",
      "ID:62b7abc3-508d-4582-b795-d4f8681100eb | NAME:csv_query\n"
     ]
    }
   ],
   "source": [
    "# Check active streams\n",
    "for s in spark.streams.active:\n",
    "    print(\"ID:{} | NAME:{}\".format(s.id, s.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
